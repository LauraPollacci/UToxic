{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed12b4f",
   "metadata": {},
   "source": [
    "# Feature dei commenti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffb17f",
   "metadata": {},
   "source": [
    "Creation of new features for the comment, considering both grammar and sentiment analysis/emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d742b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#emoji analysis\n",
    "import emoji\n",
    "import emojis\n",
    "\n",
    "#time analysis\n",
    "from datetime import datetime\n",
    "\n",
    "#pulizia testo\n",
    "from html import unescape\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "#POS tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#emozioni estratte con NRCLex\n",
    "from nrclex import NRCLex\n",
    "\n",
    "#test di leggibilità\n",
    "from readability import Readability\n",
    "\n",
    "#polarità di Textblob\n",
    "from textblob import TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04377f1",
   "metadata": {},
   "source": [
    "## File JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71018593",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [\"...\"] \n",
    "folder = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203edb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lines_def = []\n",
    "for i in range(0, len(json_files)):\n",
    "    with open(folder+json_files[i], 'r') as incsv:\n",
    "        for line in incsv:\n",
    "            lines_def.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41564e01",
   "metadata": {},
   "source": [
    "# emoji analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5919172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract emoji information (number, set and list of emojis)\n",
    "def extract_emojis(listLine):\n",
    "    for i in listLine:\n",
    "        count_e = emoji.emoji_count(i[\"text\"])\n",
    "        set_e = ''.join(emoji.distinct_emoji_list(i[\"text\"]))\n",
    "        data_list_e = emoji.emoji_list(i[\"text\"])\n",
    "        if data_list_e:\n",
    "            str_e = ''.join([entry['emoji'] for entry in data_list_e])\n",
    "        else:\n",
    "            str_e = ''\n",
    "\n",
    "        standard_s = emojis.decode(i[\"text\"]) # the emojis are decoded (e.g. the smiley face becomes :smile:)\n",
    "        i[\"text\"] = standard_s\n",
    "        i[\"emoji_count\"] = count_e\n",
    "        i[\"emoji_unique\"] = set_e\n",
    "        i[\"emoji_list\"] = str_e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2cb832",
   "metadata": {},
   "source": [
    "# time handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcdb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes the key time, converts it into date and time_of day, adds these two keys and then removes the key time\n",
    "def convert_time(listLine):\n",
    "    for i in listLine:\n",
    "        date = datetime.fromtimestamp(i[\"time\"])\n",
    "        i[\"date\"] = date.date().strftime(\"%Y-%m-%d\")\n",
    "        i[\"time_of_day\"] = date.time().strftime(\"%H:%M:%S\")\n",
    "        del i[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d79b5",
   "metadata": {},
   "source": [
    "# text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83107129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean a single text\n",
    "def clean_single_text(listLine):\n",
    "    for i in listLine:\n",
    "      # Decode HTML entities\n",
    "      i[\"text\"] = unescape(i[\"text\"])\n",
    "\n",
    "      # Remove URLs\n",
    "      i[\"text\"] = re.sub(r'https?://\\S+|www\\.\\S+', '', i[\"text\"])\n",
    "\n",
    "      # Remove symbols excluding numbers and punctuation\n",
    "      i[\"text\"] = re.sub(r'[^a-zA-Z0-9\\s' + re.escape(string.punctuation) + ']', '', i[\"text\"])\n",
    "\n",
    "      # Reduce multiple spaces to one\n",
    "      i[\"text\"] = re.sub(r'\\s+', ' ', i[\"text\"])\n",
    "\n",
    "      # Remove new lines and tabs\n",
    "      i[\"text\"] = re.sub(r'[\\n\\t]', ' ', i[\"text\"])\n",
    "\n",
    "      i[\"text\"] = i[\"text\"].strip()  # Remove leading and trailing spaces\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18422a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to expand contraction (it's => it is). slang is set to True as it is possible to find slang words in reddit posts\n",
    "def expand_contractions(listLine):\n",
    "    for i in listLine:\n",
    "        i[\"text\"] = contractions.fix(i[\"text\"], slang=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af653c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to define number of unique words and uppercase words in the text\n",
    "def unique_upper_words(listLine):\n",
    "    for i in listLine:\n",
    "        # number of unique words in the text\n",
    "        i[\"num_unique_words\"] = len(set(str(i['text']).lower().split()))\n",
    "\n",
    "        # number of Upper case words in the text\n",
    "        i[\"num_words_upper\"] = len([w for w in str(i[\"text\"]).split() if w.isupper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IN CASE: remove punctuation and trasform text in lowercase\n",
    "my_punct = ['\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',',\n",
    "           '/', ':', ';', '<', '=', '>', '@', '[', '\\\\', ']', '^', '_',\n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', '#', '!', '?','.',':']\n",
    "\n",
    "punct_pattern = re.compile(\"[\" + re.escape(\"\".join(my_punct)) + \"]\")\n",
    "#function that removes punctuation\n",
    "def remove_punct(listLine):\n",
    "    for i in listLine:\n",
    "        i[\"text\"] = re.sub(punct_pattern, ' ', i[\"text\"])\n",
    "        i[\"text\"] = i[\"text\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8ce37",
   "metadata": {},
   "source": [
    "# PoS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a956f56",
   "metadata": {},
   "source": [
    "PoS Tagging used to define Nouns, Adjectives and Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d296d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS Tagging to determine the number of \"full words\" (Verbs, Nouns and Adj) in a post\n",
    "def posTag(listLine):\n",
    "    for i in listLine:\n",
    "        tokens = word_tokenize(i[\"text\"])\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        # Counting the full words (Adjectives, Nouns, Verbs) JJ, NN, VB\n",
    "        number_full_words = sum(1 for _, tag in pos_tags if tag.startswith((\"JJ\", \"NN\", \"VB\")))\n",
    "        i[\"number_full_words\"] = number_full_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a477509",
   "metadata": {},
   "source": [
    "# NRC LEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697485e",
   "metadata": {},
   "source": [
    "Library used to define the values of 8 emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract emotion vectors using NRCLex\n",
    "def get_emotion_vector(listLine):\n",
    "    for i in listLine:\n",
    "        emotion = NRCLex(i[\"text\"])\n",
    "        affect_frequencies_dict = {emotion_class: round(frequency, 2) for emotion_class, frequency in emotion.affect_frequencies.items()}\n",
    "        for k, v in affect_frequencies_dict.items():\n",
    "            i[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4eb27",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e35746",
   "metadata": {},
   "source": [
    "TextBlob's polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75639f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(listLine):\n",
    "    for i in listLine:\n",
    "        try:\n",
    "            blob = TextBlob(i[\"text\"])\n",
    "            i[\"polarity\"] = blob.sentiment.polarity\n",
    "        except:\n",
    "            i[\"polarity\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e29f2",
   "metadata": {},
   "source": [
    "# VAD lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b6d60",
   "metadata": {},
   "source": [
    "Extraction of the values of Arousal, Dominance and Valence according to VAD lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file VAD translated into python dictionaries \n",
    "VAD = pd.read_csv('../NRC/NRC-VAD-Lexicon.txt', sep=\"\\t\", header=None)\n",
    "VAD.columns = [\"word\", \"valence\", \"arousal\", \"dominance\"]\n",
    "VAD_dict = VAD.set_index('word').T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction of V A D values\n",
    "def emotion_VAD(text, dim):\n",
    "    words_VAD = text.split()\n",
    "    score = [VAD_dict[i][dim] if i in VAD_dict else 0 for i in words_VAD]\n",
    "    return sum(score) / max(len(score), 1)\n",
    "\n",
    "def analyze_valence(text):\n",
    "    return emotion_VAD(text, 'valence')\n",
    "\n",
    "def analyze_arousal(text):\n",
    "    return emotion_VAD(text, 'arousal')\n",
    "\n",
    "def analyze_dominance(text):\n",
    "    return emotion_VAD(text, 'dominance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VAD(listLine):\n",
    "    for i in listLine:\n",
    "        i[\"valence\"] = analyze_valence(i[\"text\"])\n",
    "        i[\"arousal\"] = analyze_arousal(i[\"text\"])\n",
    "        i[\"dominance\"] = analyze_dominance(i[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ebb00",
   "metadata": {},
   "source": [
    "# Readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa000f53",
   "metadata": {},
   "source": [
    "Values of readability according to 9 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_tests(listLine):\n",
    "    for i in listLine:\n",
    "        text_str = ''.join(i[\"text\"])\n",
    "        i[\"flesch_kincaid\"] = 0\n",
    "        i[\"flesch\"] = 0\n",
    "        i[\"fog\"] = 0\n",
    "        i[\"coleman_liau\"] = 0\n",
    "        i[\"dale_chall\"] = 0\n",
    "        i[\"ari\"] = 0\n",
    "        i[\"linsear_write\"] = 0\n",
    "        i[\"smog\"] = 0\n",
    "        i[\"spache\"] = 0\n",
    "        try:\n",
    "            r = Readability(text_str)\n",
    "            sentences = sent_tokenize(text_str)\n",
    "            # Check if the sentence count is less than 30 for SMOG calculation\n",
    "            num_sentences = len(sentences)\n",
    "\n",
    "            # Tokenize words using nltk for flesch_kincaid()\n",
    "            wordlst = text_str.split()\n",
    "            num_words = len(wordlst)\n",
    "            #print(num_sentences)\n",
    "            #print(num_words)\n",
    "            if num_words >= 100:\n",
    "                i[\"flesch_kincaid\"] = r.flesch_kincaid().score\n",
    "                i[\"flesch\"] = r.flesch().score\n",
    "                i[\"fog\"] = r.gunning_fog().score\n",
    "                i[\"coleman_liau\"] = r.coleman_liau().score\n",
    "                i[\"dale_chall\"] = r.dale_chall().score\n",
    "                i[\"ari\"] =r.ari().score\n",
    "                i[\"linsear_write\"] =r.linsear_write().score\n",
    "                if num_sentences >= 30:\n",
    "                    i[\"smog\"] =r.smog().score \n",
    "                i[\"spache\"] =r.spache().score\n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66fe3e",
   "metadata": {},
   "source": [
    "# Apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_emojis(lines_def)\n",
    "print(\"femoji\")\n",
    "convert_time(lines_def)\n",
    "print(\"time\")\n",
    "clean_single_text(lines_def)\n",
    "print(\"clean\")\n",
    "expand_contractions(lines_def)\n",
    "print(\"contr\")\n",
    "unique_upper_words(lines_def)\n",
    "print(\"upper\")\n",
    "remove_punct(lines_def)\n",
    "print(\"punct\")\n",
    "posTag(lines_def)\n",
    "print(\"POS\")\n",
    "get_emotion_vector(lines_def)\n",
    "print(\"emotion\")\n",
    "readability_tests(lines_def)\n",
    "print(\"readability\")\n",
    "get_polarity(lines_def)\n",
    "print(\"polarity\")\n",
    "get_VAD(lines_def)\n",
    "print(\"VAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2c8d5",
   "metadata": {},
   "source": [
    "Turn into CSV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(lines_def, orient='columns')\n",
    "df.to_csv('../file_csv/PIANO_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
